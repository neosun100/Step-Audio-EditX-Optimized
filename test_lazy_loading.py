#!/usr/bin/env python3
"""
æµ‹è¯•æ‡’åŠ è½½åŠŸèƒ½
"""
import time
import torch
from lazy_model_manager import LazyModelManager


def dummy_model_factory():
    """æ¨¡æ‹Ÿæ¨¡å‹å·¥å‚"""
    print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")
    time.sleep(2)  # æ¨¡æ‹ŸåŠ è½½æ—¶é—´
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼ˆå ç”¨ä¸€äº›æ˜¾å­˜ï¼‰
    model = torch.nn.Linear(1000, 1000).cuda()
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    return model


def test_lazy_loading():
    """æµ‹è¯•æ‡’åŠ è½½"""
    print("=" * 80)
    print("æµ‹è¯•æ‡’åŠ è½½ç®¡ç†å™¨")
    print("=" * 80)
    
    # åˆ›å»ºç®¡ç†å™¨
    print("\n1. åˆ›å»ºæ‡’åŠ è½½ç®¡ç†å™¨ï¼ˆç©ºé—²è¶…æ—¶: 10ç§’ï¼‰")
    manager = LazyModelManager(
        model_factory=dummy_model_factory,
        idle_timeout=10,
        auto_unload=True
    )
    
    # æ£€æŸ¥åˆå§‹çŠ¶æ€
    print("\n2. æ£€æŸ¥åˆå§‹çŠ¶æ€")
    status = manager.get_status()
    print(f"   æ¨¡å‹å·²åŠ è½½: {status['loaded']}")
    if torch.cuda.is_available():
        print(f"   GPUæ˜¾å­˜: {status.get('gpu_memory_allocated_gb', 0):.2f} GB")
    
    # é¦–æ¬¡è·å–æ¨¡å‹ï¼ˆè§¦å‘åŠ è½½ï¼‰
    print("\n3. é¦–æ¬¡è·å–æ¨¡å‹ï¼ˆè§¦å‘åŠ è½½ï¼‰")
    model = manager.get_model()
    print(f"   æ¨¡å‹ç±»å‹: {type(model)}")
    
    # æ£€æŸ¥åŠ è½½åçŠ¶æ€
    print("\n4. æ£€æŸ¥åŠ è½½åçŠ¶æ€")
    status = manager.get_status()
    print(f"   æ¨¡å‹å·²åŠ è½½: {status['loaded']}")
    if torch.cuda.is_available():
        print(f"   GPUæ˜¾å­˜: {status.get('gpu_memory_allocated_gb', 0):.2f} GB")
    
    # å†æ¬¡è·å–æ¨¡å‹ï¼ˆä¸ä¼šé‡æ–°åŠ è½½ï¼‰
    print("\n5. å†æ¬¡è·å–æ¨¡å‹ï¼ˆä¸ä¼šé‡æ–°åŠ è½½ï¼‰")
    model2 = manager.get_model()
    print(f"   æ˜¯åŒä¸€ä¸ªæ¨¡å‹: {model is model2}")
    
    # ç­‰å¾…ç©ºé—²è¶…æ—¶
    print("\n6. ç­‰å¾…ç©ºé—²è¶…æ—¶ï¼ˆ10ç§’ï¼‰...")
    for i in range(10):
        time.sleep(1)
        status = manager.get_status()
        if status['loaded']:
            idle_time = status.get('idle_time', 0)
            time_until_unload = status.get('time_until_unload', 0)
            print(f"   [{i+1}/10] ç©ºé—²æ—¶é—´: {idle_time:.1f}ç§’, è·ç¦»å¸è½½: {time_until_unload:.1f}ç§’")
        else:
            print(f"   [{i+1}/10] æ¨¡å‹å·²å¸è½½")
            break
    
    # ç­‰å¾…å¸è½½å®Œæˆ
    time.sleep(2)
    
    # æ£€æŸ¥å¸è½½åçŠ¶æ€
    print("\n7. æ£€æŸ¥å¸è½½åçŠ¶æ€")
    status = manager.get_status()
    print(f"   æ¨¡å‹å·²åŠ è½½: {status['loaded']}")
    if torch.cuda.is_available():
        print(f"   GPUæ˜¾å­˜: {status.get('gpu_memory_allocated_gb', 0):.2f} GB")
    
    # å†æ¬¡è·å–æ¨¡å‹ï¼ˆè§¦å‘é‡æ–°åŠ è½½ï¼‰
    print("\n8. å†æ¬¡è·å–æ¨¡å‹ï¼ˆè§¦å‘é‡æ–°åŠ è½½ï¼‰")
    model3 = manager.get_model()
    print(f"   æ¨¡å‹ç±»å‹: {type(model3)}")
    
    # æ‰‹åŠ¨å¸è½½
    print("\n9. æ‰‹åŠ¨å¸è½½æ¨¡å‹")
    manager.force_unload()
    status = manager.get_status()
    print(f"   æ¨¡å‹å·²åŠ è½½: {status['loaded']}")
    
    # å…³é—­ç®¡ç†å™¨
    print("\n10. å…³é—­ç®¡ç†å™¨")
    manager.shutdown()
    
    print("\n" + "=" * 80)
    print("æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    if not torch.cuda.is_available():
        print("âš ï¸  è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°CUDAï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼æµ‹è¯•")
    
    test_lazy_loading()
