# FunASR éŸ³é¢‘ç¼–ç å™¨ä¼˜åŒ–æŒ‡å—

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

æ ¹æ®å®é™…æµ‹è¯•ï¼Œ**FunASR éŸ³é¢‘ç¼–ç å™¨å ç”¨ 83%ï¼ˆ~20sï¼‰çš„å¤„ç†æ—¶é—´**ï¼Œè¿™æ˜¯çœŸæ­£çš„æ€§èƒ½ç“¶é¢ˆï¼

```
å®Œæ•´ Clone æµç¨‹ï¼ˆ24s æ€»è®¡ï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  âœ… éŸ³é¢‘ç¼–ç  (FunASR):      ~20s  (83%)  â† ä¼˜åŒ–é‡ç‚¹ï¼
  ğŸŸ¢ LLM ç”Ÿæˆ:               ~2s  (8%)
  ğŸŸ¢ éŸ³é¢‘è§£ç  (CosyVoice):   ~2s  (8%)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

**ä¼˜åŒ– FunASR æ¯”ä¼˜åŒ– LLM æ›´æœ‰ä»·å€¼ï¼**

å¦‚æœèƒ½å°† FunASR ä» 20s ä¼˜åŒ–åˆ° 10sï¼š
- æ€»æ—¶é—´ï¼š24s â†’ 14sï¼ˆæé€Ÿ **42%**ï¼‰
- æ•ˆæœè¿œè¶…ä»»ä½• LLM é‡åŒ–æ–¹æ¡ˆ

---

## ğŸ” å½“å‰å®ç°åˆ†æ

### 1. FunASR ç¼–ç æµç¨‹

```python
# tokenizer.py: StepAudioTokenizer.wav2token()
def wav2token(self, audio, sample_rate):
    # æ­¥éª¤ 1: é¢„å¤„ç†éŸ³é¢‘ï¼ˆresample, trim, normalizeï¼‰
    audio = self.preprocess_wav(audio, sample_rate)
    
    # æ­¥éª¤ 2: VQ02 ç¼–ç ï¼ˆFunASR Paraformer Encoderï¼‰â† ç“¶é¢ˆ 1
    vq02_ori = self.get_vq02_code(audio)       # ~10-12s
    
    # æ­¥éª¤ 3: VQ06 ç¼–ç ï¼ˆWhisper Mel + ONNXï¼‰â† ç“¶é¢ˆ 2
    vq06_ori = self.get_vq06_code(audio)       # ~8-10s
    
    # æ­¥éª¤ 4: åˆå¹¶ token
    return merge_tokens(vq02, vq06)
```

### 2. æ€§èƒ½ç“¶é¢ˆå®šä½

| ç»„ä»¶ | è€—æ—¶ä¼°è®¡ | å æ¯” | å®ç°æ–¹å¼ |
|------|---------|------|----------|
| **VQ02 (FunASR)** | ~10-12s | 50% | PyTorch æ¨¡å‹æ¨ç† |
| **VQ06 (Whisper)** | ~8-10s | 40% | ONNX Runtime |
| é¢„å¤„ç† | ~1s | 5% | CPU æ“ä½œ |
| Token åˆå¹¶ | ~1s | 5% | CPU æ“ä½œ |

### 3. å½“å‰é…ç½®

```python
# VQ02: FunASR Paraformer
self.funasr_model = AutoModel(
    model="dengcunqin/speech_paraformer-large_asr_nat-zh-cantonese-en-16k-vocab8501-online"
)
# ä½¿ç”¨ chunk streaming æ¨¡å¼
chunk_size = [0, 4, 5]

# VQ06: Whisper + ONNX
self.ort_session = onnxruntime.InferenceSession(
    "speech_tokenizer_v1.onnx",
    providers=["CUDAExecutionProvider"]
)
# ä¼˜åŒ–å·²å¯ç”¨ï¼š
# - graph_optimization_level = ORT_ENABLE_ALL
# - intra_op_num_threads = 1
```

---

## ğŸš€ ä¼˜åŒ–æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: ONNX åŒ– FunASR æ¨¡å‹ â­â­â­â­â­

**åŸç†**ï¼šå°† PyTorch FunASR æ¨¡å‹è½¬æ¢ä¸º ONNXï¼Œä½¿ç”¨ TensorRT åŠ é€Ÿ

**é¢„æœŸæé€Ÿ**ï¼š2-3xï¼ˆ10s â†’ 3-5sï¼‰

**å®æ–½æ­¥éª¤**ï¼š

```bash
# 1. å¯¼å‡º FunASR æ¨¡å‹ä¸º ONNX
python export_funasr_to_onnx.py \
    --model-path /model/Step-Audio-Tokenizer/dengcunqin/... \
    --output funasr_encoder.onnx

# 2. ä½¿ç”¨ TensorRT ä¼˜åŒ– ONNX æ¨¡å‹
trtexec --onnx=funasr_encoder.onnx \
    --saveEngine=funasr_encoder.trt \
    --fp16 \
    --workspace=4096

# 3. ä¿®æ”¹ tokenizer.py ä½¿ç”¨ TensorRT å¼•æ“
```

**ä¼˜ç‚¹**ï¼š
- âœ… æ˜¾è‘—æé€Ÿï¼ˆ2-3xï¼‰
- âœ… ä¸æ”¹å˜æ¨¡å‹ç²¾åº¦
- âœ… GPU åˆ©ç”¨ç‡æ›´é«˜

**ç¼ºç‚¹**ï¼š
- âŒ éœ€è¦é¢å¤–çš„æ¨¡å‹è½¬æ¢å·¥ä½œ
- âŒ TensorRT å¼•æ“ä¸è·¨ GPU æ¶æ„é€šç”¨

**éš¾åº¦**ï¼šä¸­ç­‰

---

### æ–¹æ¡ˆ 2: æ‰¹å¤„ç†ä¼˜åŒ– â­â­â­â­

**åŸç†**ï¼šæ”¯æŒ batch inferenceï¼Œä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªéŸ³é¢‘

**é¢„æœŸæé€Ÿ**ï¼š1.5-2xï¼ˆå•ä¸ªè¯·æ±‚ä¸å˜ï¼Œååé‡æå‡ï¼‰

**å®æ–½æ­¥éª¤**ï¼š

```python
# ä¿®æ”¹ tokenizer.py: StepAudioTokenizer
class StepAudioTokenizer:
    def wav2token_batch(self, audios: list[torch.Tensor], sample_rates: list[int]):
        """æ‰¹å¤„ç†ç‰ˆæœ¬"""
        # 1. æ‰¹é‡é¢„å¤„ç†
        preprocessed = [
            self.preprocess_wav(audio, sr) 
            for audio, sr in zip(audios, sample_rates)
        ]
        
        # 2. Pad åˆ°ç›¸åŒé•¿åº¦
        max_len = max(a.shape[-1] for a in preprocessed)
        padded = torch.stack([
            torch.nn.functional.pad(a, (0, max_len - a.shape[-1]))
            for a in preprocessed
        ])
        
        # 3. æ‰¹é‡æ¨ç† VQ02
        vq02_batch = self.funasr_model.infer_encoder(
            input=padded,  # (batch_size, audio_len)
            batch_size=len(audios)
        )
        
        # 4. æ‰¹é‡æ¨ç† VQ06
        mel_features = torch.stack([
            whisper.log_mel_spectrogram(audio)
            for audio in preprocessed
        ])
        vq06_batch = self.ort_session.run(None, {
            "mel_features": mel_features.numpy()
        })
        
        return zip(vq02_batch, vq06_batch)
```

**ä¼˜ç‚¹**ï¼š
- âœ… æé«˜ååé‡
- âœ… æ›´å¥½çš„ GPU åˆ©ç”¨ç‡
- âœ… é€‚åˆ API æ‰¹é‡è¯·æ±‚

**ç¼ºç‚¹**ï¼š
- âŒ å•ä¸ªè¯·æ±‚å»¶è¿Ÿä¸å˜
- âŒ éœ€è¦è¯·æ±‚é˜Ÿåˆ—å’Œè°ƒåº¦å™¨

**éš¾åº¦**ï¼šä¸­ç­‰

---

### æ–¹æ¡ˆ 3: ä½¿ç”¨æ›´è½»é‡çš„ç¼–ç å™¨ â­â­â­â­â­

**åŸç†**ï¼šæ›¿æ¢ `paraformer-large` ä¸ºæ›´å°/æ›´å¿«çš„æ¨¡å‹

**å€™é€‰æ¨¡å‹**ï¼š

| æ¨¡å‹ | å‚æ•°é‡ | é¢„æœŸé€Ÿåº¦ | ç²¾åº¦æŸå¤± |
|------|--------|---------|---------|
| **paraformer-base** | 220M | 2-3x æ›´å¿« | è½»å¾® |
| **whisper-small** | 244M | 3-4x æ›´å¿« | ä¸­ç­‰ |
| **whisper-tiny** | 39M | 5-6x æ›´å¿« | è¾ƒå¤§ |

**å®æ–½æ­¥éª¤**ï¼š

```python
# 1. ä¿®æ”¹ tokenizer.py åˆå§‹åŒ–å‚æ•°
class StepAudioTokenizer:
    def __init__(
        self,
        encoder_path,
        funasr_model_id="dengcunqin/speech_paraformer-base-asr_nat-zh-cn-16k-common-vocab8404"  # æ”¹ç”¨ base
    ):
        self.funasr_model = AutoModel(model=funasr_model_id)
        ...

# 2. æµ‹è¯•ç²¾åº¦
python benchmark_encoder_accuracy.py \
    --original-model paraformer-large \
    --candidate-model paraformer-base \
    --test-audio examples/*.wav
```

**ä¼˜ç‚¹**ï¼š
- âœ… æœ€ç®€å•ï¼ˆåªéœ€ä¿®æ”¹é…ç½®ï¼‰
- âœ… æ˜¾è‘—æé€Ÿï¼ˆ2-4xï¼‰
- âœ… å†…å­˜å ç”¨æ›´å°

**ç¼ºç‚¹**ï¼š
- âŒ å¯èƒ½å½±å“éŸ³é¢‘è´¨é‡
- âŒ éœ€è¦ A/B æµ‹è¯•éªŒè¯

**éš¾åº¦**ï¼šç®€å• â­

---

### æ–¹æ¡ˆ 4: CUDA Graph åŠ é€Ÿ â­â­â­

**åŸç†**ï¼šä½¿ç”¨ CUDA Graph æ•è·å›ºå®šè®¡ç®—å›¾ï¼Œå‡å°‘ kernel launch å¼€é”€

**é¢„æœŸæé€Ÿ**ï¼š1.3-1.5x

**å®æ–½æ­¥éª¤**ï¼š

```python
# ä¿®æ”¹ tokenizer.py
class StepAudioTokenizer:
    def __init__(self, ...):
        ...
        # é¢„çƒ­ + æ•è· CUDA Graph
        self.vq02_graph = None
        self.vq06_graph = None
        self._warmup_cuda_graphs()
    
    def _warmup_cuda_graphs(self):
        """é¢„çƒ­å¹¶æ•è· CUDA Graph"""
        dummy_audio = torch.randn(1, 16000 * 10).cuda()  # 10s audio
        
        # é¢„çƒ­
        for _ in range(3):
            _ = self.get_vq02_code(dummy_audio)
        
        # æ•è· VQ02 graph
        self.vq02_graph = torch.cuda.CUDAGraph()
        with torch.cuda.graph(self.vq02_graph):
            self.vq02_output = self.get_vq02_code(dummy_audio)
        
        # åŒæ ·å¤„ç† VQ06
        ...
    
    def get_vq02_code(self, audio):
        if self.vq02_graph is not None:
            # ä½¿ç”¨ CUDA Graph
            self.vq02_graph.replay()
            return self.vq02_output.clone()
        else:
            # åŸå§‹æ¨ç†
            return self._original_vq02_inference(audio)
```

**ä¼˜ç‚¹**ï¼š
- âœ… æ— ç²¾åº¦æŸå¤±
- âœ… ä¸­ç­‰æé€Ÿï¼ˆ1.3-1.5xï¼‰

**ç¼ºç‚¹**ï¼š
- âŒ åªæ”¯æŒå›ºå®šè¾“å…¥å°ºå¯¸
- âŒ å®ç°å¤æ‚

**éš¾åº¦**ï¼šé«˜

---

### æ–¹æ¡ˆ 5: æ··åˆç²¾åº¦ + Torch Compile â­â­â­â­

**åŸç†**ï¼šä½¿ç”¨ PyTorch 2.x çš„ `torch.compile()` ä¼˜åŒ–

**é¢„æœŸæé€Ÿ**ï¼š1.5-2x

**å®æ–½æ­¥éª¤**ï¼š

```python
# ä¿®æ”¹ tokenizer.py
class StepAudioTokenizer:
    def __init__(self, ...):
        ...
        # ç¼–è¯‘ FunASR æ¨¡å‹
        self.funasr_model = torch.compile(
            self.funasr_model,
            mode="reduce-overhead",  # æˆ– "max-autotune"
            fullgraph=True
        )
        
        # å¯ç”¨ TF32 åŠ é€Ÿï¼ˆAmpere+ GPUï¼‰
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
```

**ä¼˜ç‚¹**ï¼š
- âœ… ç®€å•ï¼ˆåªéœ€æ·»åŠ ä¸€è¡Œä»£ç ï¼‰
- âœ… æ— ç²¾åº¦æŸå¤±
- âœ… PyTorch åŸç”Ÿæ”¯æŒ

**ç¼ºç‚¹**ï¼š
- âŒ é¦–æ¬¡è¿è¡Œéœ€è¦ç¼–è¯‘æ—¶é—´
- âŒ éœ€è¦ PyTorch 2.0+

**éš¾åº¦**ï¼šç®€å• â­

---

### æ–¹æ¡ˆ 6: å¤š GPU å¹¶è¡Œ â­â­

**åŸç†**ï¼šå°† VQ02 å’Œ VQ06 åˆ†é…åˆ°ä¸åŒ GPU å¹¶è¡Œè®¡ç®—

**é¢„æœŸæé€Ÿ**ï¼š1.8-2xï¼ˆç†è®ºå€¼ï¼Œå®é™…å—é™äºæ•°æ®ä¼ è¾“ï¼‰

**å®æ–½æ­¥éª¤**ï¼š

```python
class StepAudioTokenizer:
    def __init__(self, encoder_path, vq02_device="cuda:0", vq06_device="cuda:1"):
        # VQ02 åœ¨ GPU 0
        self.funasr_model = AutoModel(...).to(vq02_device)
        
        # VQ06 åœ¨ GPU 1
        self.ort_session = onnxruntime.InferenceSession(
            ...,
            providers=[("CUDAExecutionProvider", {"device_id": 1})]
        )
    
    def wav2token(self, audio, sr):
        audio = self.preprocess_wav(audio, sr)
        
        # å¹¶è¡Œæ‰§è¡Œ
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            future_vq02 = executor.submit(self.get_vq02_code, audio)
            future_vq06 = executor.submit(self.get_vq06_code, audio)
            
            vq02 = future_vq02.result()
            vq06 = future_vq06.result()
        
        return self.merge_vq0206_to_token_str(vq02, vq06)
```

**ä¼˜ç‚¹**ï¼š
- âœ… å……åˆ†åˆ©ç”¨å¤š GPU
- âœ… å¹¶è¡Œè®¡ç®—ï¼Œç†è®º 2x æé€Ÿ

**ç¼ºç‚¹**ï¼š
- âŒ éœ€è¦é¢å¤–çš„ GPU
- âŒ æ•°æ®ä¼ è¾“å¼€é”€
- âŒ ä¸é€‚åˆå• GPU ç”¨æˆ·

**éš¾åº¦**ï¼šä¸­ç­‰

---

## ğŸ“Š ä¼˜åŒ–æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | é¢„æœŸæé€Ÿ | å®æ–½éš¾åº¦ | ç²¾åº¦å½±å“ | GPU éœ€æ±‚ | æ¨èåº¦ |
|------|---------|---------|---------|---------|--------|
| **ONNX + TensorRT** | 2-3x | ä¸­ | æ—  | 1 GPU | â­â­â­â­â­ |
| **è½»é‡æ¨¡å‹** | 2-4x | ä½ | è½»å¾® | 1 GPU | â­â­â­â­â­ |
| **Torch Compile** | 1.5-2x | ä½ | æ—  | 1 GPU | â­â­â­â­ |
| **æ‰¹å¤„ç†** | 1.5-2x | ä¸­ | æ—  | 1 GPU | â­â­â­â­ |
| **CUDA Graph** | 1.3-1.5x | é«˜ | æ—  | 1 GPU | â­â­â­ |
| **å¤š GPU å¹¶è¡Œ** | 1.8-2x | ä¸­ | æ—  | 2+ GPU | â­â­ |

---

## ğŸ¯ æ¨èå®æ–½è·¯çº¿å›¾

### é˜¶æ®µ 1: å¿«é€Ÿä¼˜åŒ–ï¼ˆ1-2 å¤©ï¼‰â­â­â­â­â­

**æ–¹æ¡ˆ**ï¼šè½»é‡æ¨¡å‹ + Torch Compile

```bash
# 1. ä¿®æ”¹é…ç½®ä½¿ç”¨ paraformer-base
vim tokenizer.py  # ä¿®æ”¹ funasr_model_id

# 2. æ·»åŠ  torch.compile()
# åœ¨ __init__ ä¸­æ·»åŠ ï¼š
self.funasr_model = torch.compile(self.funasr_model)

# 3. æµ‹è¯•
python benchmark_encoder.py --model base --compile
```

**é¢„æœŸæ•ˆæœ**ï¼š
- 20s â†’ 8-10sï¼ˆæé€Ÿ **2-2.5x**ï¼‰
- æ€»æ—¶é—´ï¼š24s â†’ 12-14sï¼ˆæé€Ÿ **42-50%**ï¼‰

---

### é˜¶æ®µ 2: æ·±åº¦ä¼˜åŒ–ï¼ˆ1-2 å‘¨ï¼‰â­â­â­â­â­

**æ–¹æ¡ˆ**ï¼šONNX + TensorRT

```bash
# 1. å¯¼å‡º ONNX
python scripts/export_funasr_onnx.py

# 2. TensorRT ä¼˜åŒ–
trtexec --onnx=funasr.onnx --saveEngine=funasr.trt --fp16

# 3. é›†æˆ TensorRT å¼•æ“
python scripts/integrate_tensorrt.py

# 4. æµ‹è¯•
python benchmark_encoder.py --engine tensorrt
```

**é¢„æœŸæ•ˆæœ**ï¼š
- 20s â†’ 6-8sï¼ˆæé€Ÿ **2.5-3.3x**ï¼‰
- æ€»æ—¶é—´ï¼š24s â†’ 10-12sï¼ˆæé€Ÿ **50-58%**ï¼‰

---

### é˜¶æ®µ 3: ç”Ÿäº§ä¼˜åŒ–ï¼ˆæŒç»­ï¼‰â­â­â­â­

**æ–¹æ¡ˆ**ï¼šæ‰¹å¤„ç† + è¯·æ±‚é˜Ÿåˆ—

```bash
# 1. å®ç°æ‰¹å¤„ç† API
vim api_server.py  # æ·»åŠ è¯·æ±‚é˜Ÿåˆ—

# 2. åŠ¨æ€æ‰¹å¤„ç†
python api_server.py \
    --enable-batching \
    --max-batch-size 4 \
    --batch-timeout 100ms

# 3. å‹æµ‹
locust -f load_test.py --host http://localhost:8003
```

**é¢„æœŸæ•ˆæœ**ï¼š
- ååé‡ï¼š1 req/s â†’ 3-4 req/sï¼ˆæé€Ÿ **3-4x**ï¼‰
- é€‚åˆé«˜å¹¶å‘åœºæ™¯

---

## ğŸ› ï¸ ç«‹å³å¯åšçš„ä¼˜åŒ–

### 1. å¯ç”¨ TF32ï¼ˆ30 ç§’å®Œæˆï¼‰â­â­â­â­â­

```python
# åœ¨ tokenizer.py __init__ å¼€å¤´æ·»åŠ ï¼š
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
```

**é¢„æœŸæé€Ÿ**ï¼š1.1-1.2xï¼ˆå…è´¹çš„æ€§èƒ½ï¼‰

### 2. å‡å°‘ä¸å¿…è¦çš„æ•°æ®ä¼ è¾“

```python
# å½“å‰å®ç°ï¼ˆæ…¢ï¼‰:
audio = audio.cpu().numpy()  # GPU â†’ CPU
audio = trim_silence(audio)
audio = torch.from_numpy(audio).cuda()  # CPU â†’ GPU

# ä¼˜åŒ–åï¼ˆå¿«ï¼‰:
if audio.is_cuda:
    audio = trim_silence_gpu(audio)  # å…¨ç¨‹ GPU
```

### 3. ä½¿ç”¨æ›´ä¼˜çš„ ONNX Runtime é…ç½®

```python
# å½“å‰ï¼š
session_option.intra_op_num_threads = 1  # å¤ªä¿å®ˆï¼

# ä¼˜åŒ–ï¼š
session_option.intra_op_num_threads = 4
session_option.inter_op_num_threads = 2
session_option.execution_mode = onnxruntime.ExecutionMode.ORT_PARALLEL
```

---

## ğŸ“ˆ é¢„æœŸæ€»ä½“ä¼˜åŒ–æ•ˆæœ

```
å½“å‰åŸºçº¿ï¼ˆBase æ¨¡å‹ï¼‰ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  éŸ³é¢‘ç¼–ç :  20s  (83%)
  LLM ç”Ÿæˆ:   2s  (8%)
  éŸ³é¢‘è§£ç :   2s  (8%)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  æ€»è®¡:      24s  (100%)

é˜¶æ®µ 1 ä¼˜åŒ–ï¼ˆè½»é‡æ¨¡å‹ + Torch Compileï¼‰ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  éŸ³é¢‘ç¼–ç :   8s  (67%)  â† æé€Ÿ 2.5x
  LLM ç”Ÿæˆ:   2s  (17%)
  éŸ³é¢‘è§£ç :   2s  (17%)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  æ€»è®¡:      12s  â† æé€Ÿ 50% âœ…

é˜¶æ®µ 2 ä¼˜åŒ–ï¼ˆONNX + TensorRTï¼‰ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  éŸ³é¢‘ç¼–ç :   6s  (60%)  â† æé€Ÿ 3.3x
  LLM ç”Ÿæˆ:   2s  (20%)
  éŸ³é¢‘è§£ç :   2s  (20%)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  æ€»è®¡:      10s  â† æé€Ÿ 58% âœ…âœ…

ç†æƒ³æƒ…å†µï¼ˆæ‰€æœ‰ä¼˜åŒ–ï¼‰ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  éŸ³é¢‘ç¼–ç :   4s  (50%)  â† æé€Ÿ 5x
  LLM ç”Ÿæˆ:   2s  (25%)
  éŸ³é¢‘è§£ç :   2s  (25%)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  æ€»è®¡:       8s  â† æé€Ÿ 66% âœ…âœ…âœ…
```

---

## ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³æ‰§è¡Œï¼ˆä»Šå¤©ï¼‰ï¼š

1. **å¯ç”¨ TF32**ï¼ˆ30 ç§’ï¼‰
   ```python
   # tokenizer.py:18 (åœ¨ import å)
   torch.backends.cuda.matmul.allow_tf32 = True
   torch.backends.cudnn.allow_tf32 = True
   ```

2. **æµ‹è¯•è½»é‡æ¨¡å‹**ï¼ˆ30 åˆ†é’Ÿï¼‰
   ```bash
   # ä¸‹è½½ paraformer-base
   git lfs clone https://huggingface.co/damo/speech_paraformer-base_asr_nat-zh-cn-16k-common-vocab8404
   
   # ä¿®æ”¹é…ç½®
   vim tokenizer.py  # æ”¹ model_id
   
   # æµ‹è¯•
   python test_clone.py --measure-time
   ```

### æœ¬å‘¨å®Œæˆï¼š

3. **æ·»åŠ  Torch Compile**ï¼ˆ1 å¤©ï¼‰
4. **ä¼˜åŒ– ONNX Runtime é…ç½®**ï¼ˆ1 å¤©ï¼‰
5. **æ€§èƒ½åŸºå‡†æµ‹è¯•**ï¼ˆ1 å¤©ï¼‰

### ä¸‹å‘¨å¼€å§‹ï¼š

6. **ONNX å¯¼å‡º + TensorRT ä¼˜åŒ–**ï¼ˆ1-2 å‘¨ï¼‰
7. **æ‰¹å¤„ç†å®ç°**ï¼ˆ1 å‘¨ï¼‰

---

## ğŸ“ æ€»ç»“

**å…³é”®æ´å¯Ÿ**ï¼š
- âœ… FunASR ç¼–ç å  83% æ—¶é—´ï¼Œæ˜¯çœŸæ­£çš„ç“¶é¢ˆ
- âœ… ä¼˜åŒ–ç¼–ç å™¨æ¯”ä¼˜åŒ– LLM æ›´æœ‰ä»·å€¼ï¼ˆ10xï¼‰
- âœ… æœ€ç®€å•çš„æ–¹æ¡ˆï¼ˆè½»é‡æ¨¡å‹ï¼‰å°±èƒ½æé€Ÿ 2-3x
- âœ… ç»“åˆå¤šç§æ–¹æ¡ˆï¼Œç†è®ºå¯æé€Ÿ 5x+

**æ¨èè·¯å¾„**ï¼š
1. å…ˆç”¨è½»é‡æ¨¡å‹å¿«é€ŸéªŒè¯æ•ˆæœ
2. ç¡®è®¤ç²¾åº¦å¯æ¥å—åï¼Œéƒ¨ç½²åˆ°ç”Ÿäº§
3. é•¿æœŸæŠ•å…¥ TensorRT ä¼˜åŒ–è·å¾—æœ€ä½³æ€§èƒ½

**æŠ•èµ„å›æŠ¥**ï¼š
- é˜¶æ®µ 1ï¼š2 å¤©å·¥ä½œ â†’ 50% æ€§èƒ½æå‡
- é˜¶æ®µ 2ï¼š2 å‘¨å·¥ä½œ â†’ 60% æ€§èƒ½æå‡
- ROI æé«˜ï¼ğŸš€
