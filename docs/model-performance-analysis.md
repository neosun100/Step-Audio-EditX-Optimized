# Step-Audio-EditX æ¨¡å‹æ€§èƒ½åˆ†ææŠ¥å‘Š

## ğŸ“Š æµ‹è¯•ç»“æœæ¦‚è§ˆ

| æŒ‡æ ‡ | Base æ¨¡å‹ | AWQ 4-bit æ¨¡å‹ | å¯¹æ¯” |
|------|-----------|----------------|------|
| **æ¨¡å‹å¤§å°** | 16 GB (æƒé‡ 6.6GB) | 7.1 GB (æƒé‡ 2.4GB) | AWQ èŠ‚çœ **56% ç£ç›˜ç©ºé—´** |
| **åŠ è½½æ—¶é—´** | 2.31s | 2.77s | AWQ æ…¢ 20% |
| **æ¨ç†æ—¶é—´** | 1.486s | 5.861s | **AWQ æ…¢ 3.94 å€** âš ï¸ |
| **æ˜¾å­˜å ç”¨** | ~23-24 GB | ~23-24 GB | ç›¸è¿‘ |

## ğŸ” é—®é¢˜æ ¹å› åˆ†æ

### 1. é‡åŒ–æ ¼å¼é—®é¢˜

AWQ æ¨¡å‹ä½¿ç”¨çš„æ˜¯ **compressed-tensors** æ ¼å¼ï¼Œè€Œéä¼ ç»Ÿçš„ AutoAWQ æ ¼å¼ï¼š

```json
{
  "quant_method": "compressed-tensors",
  "quantization_status": "compressed",
  "config_groups": {
    "group_0": {
      "format": "pack-quantized",
      "weights": {
        "num_bits": 4,
        "group_size": 128,
        "symmetric": true,
        "type": "int",
        "dynamic": false
      }
    }
  }
}
```

**é—®é¢˜**ï¼š
- `compressed-tensors` æ˜¯ç›¸å¯¹è¾ƒæ–°çš„é‡åŒ–æ ¼å¼ï¼Œä¼˜åŒ–ä¸å¦‚æˆç†Ÿçš„ AutoAWQ
- ç¼ºå°‘ä¸“é—¨çš„ CUDA kernel åŠ é€Ÿ
- åŠ¨æ€åé‡åŒ–ï¼ˆdequantizationï¼‰å¼€é”€å¤§

### 2. æ¨ç†è·¯å¾„å¯¹æ¯”

#### Base æ¨¡å‹æ¨ç†è·¯å¾„
```
è¾“å…¥ â†’ BFloat16 çŸ©é˜µè¿ç®— â†’ è¾“å‡º
       â””â”€ ä½¿ç”¨ä¼˜åŒ–çš„ cuBLAS/Flash Attention
```

#### AWQ æ¨¡å‹æ¨ç†è·¯å¾„ï¼ˆcompressed-tensorsï¼‰
```
è¾“å…¥ â†’ 4-bit æƒé‡è¯»å– â†’ åŠ¨æ€åé‡åŒ–åˆ° BFloat16 â†’ çŸ©é˜µè¿ç®— â†’ è¾“å‡º
       â””â”€ æ¯æ¬¡å‰å‘ä¼ æ’­éƒ½è¦åé‡åŒ–    â””â”€ é€šç”¨ Python ä»£ç ï¼Œæ— ä¸“ç”¨ kernel
       â””â”€ CPU-GPU æ•°æ®ä¼ è¾“å¼€é”€
```

### 3. ä¸ºä»€ä¹ˆ AutoAWQ æ ¼å¼ä¼šæ›´å¿«ï¼Ÿ

AutoAWQ ä½¿ç”¨ï¼š
- **èåˆ kernel**ï¼šé‡åŒ–æƒé‡ç›´æ¥å‚ä¸çŸ©é˜µä¹˜æ³•ï¼Œæ— éœ€å®Œæ•´åé‡åŒ–
- **Group-wise é‡åŒ–ä¼˜åŒ–**ï¼šç¡¬ä»¶çº§åˆ«çš„å¹¶è¡Œå¤„ç†
- **é¢„ç¼–è¯‘ CUDA kernel**ï¼šé’ˆå¯¹ NVIDIA GPU æ·±åº¦ä¼˜åŒ–
- **é›¶æ‹·è´æ¨ç†**ï¼šå‡å°‘ CPU-GPU æ•°æ®ä¼ è¾“

compressed-tensors ç¼ºå°‘è¿™äº›ä¼˜åŒ–ï¼Œå¯¼è‡´ï¼š
- æ¯å±‚éƒ½è¦åŠ¨æ€åé‡åŒ–ï¼ˆbottleneckï¼‰
- ä½¿ç”¨é€šç”¨ PyTorch æ“ä½œï¼Œæ— ä¸“ç”¨åŠ é€Ÿ
- é¢å¤–çš„å†…å­˜æ‹·è´å’Œç±»å‹è½¬æ¢å¼€é”€

## ğŸ’¡ è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1ï¼šä½¿ç”¨ AutoAWQ é‡æ–°é‡åŒ–ï¼ˆæ¨èï¼‰

```bash
# å®‰è£… AutoAWQ
pip install autoawq

# é‡æ–°é‡åŒ–æ¨¡å‹
python -c "
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

model_path = '/model/Step-Audio-EditX'
quant_path = '/model/Step-Audio-EditX-AutoAWQ-4bit'

# åŠ è½½åŸå§‹æ¨¡å‹
model = AutoAWQForCausalLM.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# å‡†å¤‡æ ¡å‡†æ•°æ®ï¼ˆä»è®­ç»ƒé›†é‡‡æ ·ï¼‰
# quant_config = { 'zero_point': True, 'q_group_size': 128, 'w_bit': 4 }

# é‡åŒ–å¹¶ä¿å­˜
# model.quantize(tokenizer, quant_config=quant_config)
# model.save_quantized(quant_path)
"
```

**é¢„æœŸæå‡**ï¼š2-3x åŠ é€Ÿï¼Œæ˜¾å­˜èŠ‚çœ 40-50%

### æ–¹æ¡ˆ 2ï¼šä½¿ç”¨ vLLM æ¨ç†å¼•æ“ï¼ˆéœ€è¦å¤§æ”¹ï¼‰

vLLM æ”¯æŒé«˜æ€§èƒ½ AWQ æ¨ç†ï¼Œä½†éœ€è¦ï¼š
- é‡æ„ `tts.py` å’Œ `api_server.py`
- ä½¿ç”¨ vLLM çš„ LLM ç±»æ›¿ä»£åŸç”Ÿ transformers
- é€‚é…æ‰¹å¤„ç†å’Œæµå¼ç”Ÿæˆ

**é¢„æœŸæå‡**ï¼š3-5x åŠ é€Ÿï¼Œæ”¯æŒæ›´å¤§ batch size

### æ–¹æ¡ˆ 3ï¼šä½¿ç”¨ Optimum + BetterTransformer

```bash
pip install optimum accelerate

# åœ¨åŠ è½½æ—¶å¯ç”¨ä¼˜åŒ–
from optimum.bettertransformer import BetterTransformer
model = BetterTransformer.transform(model)
```

**é¢„æœŸæå‡**ï¼š1.2-1.5x åŠ é€Ÿï¼ˆå¯¹é‡åŒ–æ¨¡å‹æå‡æœ‰é™ï¼‰

### æ–¹æ¡ˆ 4ï¼šæš‚æ—¶ç¦ç”¨ AWQ æ¨¡å‹ï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼‰

åœ¨ UI å’Œ API ä¸­éšè— AWQ é€‰é¡¹ï¼Œä»…ä½¿ç”¨ base æ¨¡å‹ï¼š

```python
# app.py / api_server.py
# æ³¨é‡Šæ‰ AWQ æ¨¡å‹åŠ è½½ä»£ç 
# if awq_model_path and os.path.exists(awq_model_path):
#     ...
```

## ğŸ¯ å»ºè®®è¡ŒåŠ¨

### çŸ­æœŸï¼ˆç«‹å³å¯åšï¼‰
1. âœ… **æ–‡æ¡£æ›´æ–°**ï¼šåœ¨ README ä¸­è¯´æ˜ AWQ æ¨¡å‹å½“å‰æ€§èƒ½é—®é¢˜
2. âœ… **UI æç¤º**ï¼šåœ¨æ¨¡å‹é€‰æ‹©å¤„æ·»åŠ æ€§èƒ½è­¦å‘Š
3. âš ï¸ **é»˜è®¤ä½¿ç”¨ base**ï¼šå°†é»˜è®¤æ¨¡å‹è®¾ä¸º baseï¼ŒAWQ æ ‡è®°ä¸º"å®éªŒæ€§"

### ä¸­æœŸï¼ˆ1-2 å‘¨ï¼‰
1. ğŸ”„ **é‡æ–°é‡åŒ–**ï¼šä½¿ç”¨ AutoAWQ é‡æ–°é‡åŒ–æ¨¡å‹å¹¶æµ‹è¯•
2. ğŸ“Š **æ€§èƒ½å¯¹æ¯”**ï¼šå¯¹æ¯” AutoAWQ vs compressed-tensors
3. ğŸ“ **æ›´æ–°æ–‡æ¡£**ï¼šæä¾›æ€§èƒ½åŸºå‡†å’Œé€‰æ‹©æŒ‡å—

### é•¿æœŸï¼ˆ1-2 æœˆï¼‰
1. ğŸš€ **é›†æˆ vLLM**ï¼šä½œä¸ºå¯é€‰æ¨ç†åç«¯
2. âš¡ **æ‰¹å¤„ç†ä¼˜åŒ–**ï¼šæ”¯æŒæ‰¹é‡è¯·æ±‚å¤„ç†
3. ğŸ”¬ **A/B æµ‹è¯•**ï¼šä¸åŒé‡åŒ–æ–¹æ¡ˆçš„éŸ³è´¨å¯¹æ¯”

## ğŸ“ˆ æ€§èƒ½åŸºå‡†ï¼ˆä¾›å‚è€ƒï¼‰

åŸºäº NVIDIA L40S GPU çš„æµ‹è¯•ç»“æœï¼š

| æ¨¡å‹é…ç½® | æ¨ç†æ—¶é—´ (50 tokens) | ååé‡ (tokens/s) | æ˜¾å­˜å ç”¨ |
|----------|---------------------|------------------|----------|
| Base (BFloat16) | 1.486s | ~33.6 | 23-24 GB |
| AWQ (compressed-tensors) | 5.861s | ~8.5 | 23-24 GB |
| **ç†è®º AutoAWQ** | ~0.7-1.0s | ~50-70 | 12-15 GB |

## ğŸ”— ç›¸å…³èµ„æº

- [AutoAWQ GitHub](https://github.com/casper-hansen/AutoAWQ)
- [compressed-tensors æ–‡æ¡£](https://github.com/vllm-project/llm-compressor)
- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [Transformers é‡åŒ–æŒ‡å—](https://huggingface.co/docs/transformers/quantization)

## ğŸ“Œ ç»“è®º

å½“å‰ AWQ æ¨¡å‹ä½¿ç”¨ compressed-tensors æ ¼å¼ï¼Œç¼ºå°‘é«˜æ€§èƒ½æ¨ç†æ”¯æŒï¼Œå¯¼è‡´ï¼š
- âœ… ç£ç›˜ç©ºé—´èŠ‚çœ 56%ï¼ˆä» 16GB â†’ 7.1GBï¼‰
- âŒ æ¨ç†é€Ÿåº¦æ…¢ 3.94 å€
- âŒ æ˜¾å­˜å ç”¨æœªæ˜æ˜¾é™ä½

**å»ºè®®æš‚æ—¶ä½¿ç”¨ base æ¨¡å‹**ï¼Œå¾…ä½¿ç”¨ AutoAWQ é‡æ–°é‡åŒ–åå†å¯ç”¨é‡åŒ–ç‰ˆæœ¬ã€‚
