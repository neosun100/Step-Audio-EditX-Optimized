#!/usr/bin/env python3
"""
FunASR å¿«é€Ÿä¼˜åŒ–è„šæœ¬
ç«‹å³å¯ç”¨ TF32 å’Œå…¶ä»–ç®€å•ä¼˜åŒ–
"""

import os
import sys
import time
import torch
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from tokenizer import StepAudioTokenizer
from model_loader import ModelSource


def enable_tf32():
    """å¯ç”¨ TF32 åŠ é€Ÿï¼ˆAmpere+ GPUï¼‰"""
    print("ğŸš€ å¯ç”¨ TF32 åŠ é€Ÿ...")
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    print("   âœ… TF32 å·²å¯ç”¨")


def optimize_onnx_session():
    """è¿”å›ä¼˜åŒ–çš„ ONNX SessionOptions"""
    import onnxruntime
    
    print("ğŸš€ é…ç½®ä¼˜åŒ–çš„ ONNX Runtime...")
    session_option = onnxruntime.SessionOptions()
    
    # å¯ç”¨æ‰€æœ‰å›¾ä¼˜åŒ–
    session_option.graph_optimization_level = (
        onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
    )
    
    # ä½¿ç”¨æ›´å¤šçº¿ç¨‹ï¼ˆä¹‹å‰æ˜¯ 1ï¼Œå¤ªä¿å®ˆï¼‰
    session_option.intra_op_num_threads = 4
    session_option.inter_op_num_threads = 2
    
    # å¯ç”¨å¹¶è¡Œæ‰§è¡Œ
    session_option.execution_mode = onnxruntime.ExecutionMode.ORT_PARALLEL
    
    print("   âœ… ONNX Runtime ä¼˜åŒ–é…ç½®å®Œæˆ")
    print(f"   - å›¾ä¼˜åŒ–: ENABLE_ALL")
    print(f"   - intra_op çº¿ç¨‹: 4")
    print(f"   - inter_op çº¿ç¨‹: 2")
    print(f"   - æ‰§è¡Œæ¨¡å¼: PARALLEL")
    
    return session_option


def benchmark_encoder(tokenizer, test_audio_path, num_runs=3):
    """åŸºå‡†æµ‹è¯•ç¼–ç å™¨"""
    import torchaudio
    
    print(f"\nğŸ“Š åŸºå‡†æµ‹è¯•ç¼–ç å™¨ï¼ˆ{num_runs} æ¬¡è¿è¡Œï¼‰...")
    
    # åŠ è½½æµ‹è¯•éŸ³é¢‘
    audio, sr = torchaudio.load(test_audio_path)
    audio = audio.cuda()
    
    # é¢„çƒ­
    print("   é¢„çƒ­ä¸­...")
    _ = tokenizer(audio, sr)
    
    # æµ‹è¯•
    times = []
    for i in range(num_runs):
        start = time.time()
        _ = tokenizer(audio, sr)
        elapsed = time.time() - start
        times.append(elapsed)
        print(f"   è¿è¡Œ {i+1}/{num_runs}: {elapsed:.2f}s")
    
    avg_time = sum(times) / len(times)
    print(f"\n   âœ… å¹³å‡è€—æ—¶: {avg_time:.2f}s")
    
    return avg_time


def test_lightweight_model(encoder_path, test_audio):
    """æµ‹è¯•è½»é‡çº§æ¨¡å‹"""
    print("\nğŸ§ª æµ‹è¯•è½»é‡çº§æ¨¡å‹: paraformer-base...")
    
    try:
        # å°è¯•åŠ è½½ base æ¨¡å‹
        base_model_id = "damo/speech_paraformer-base_asr_nat-zh-cn-16k-common-vocab8404"
        
        tokenizer = StepAudioTokenizer(
            encoder_path=encoder_path,
            model_source=ModelSource.AUTO,
            funasr_model_id=base_model_id
        )
        
        avg_time = benchmark_encoder(tokenizer, test_audio)
        print(f"   âœ… paraformer-base å¹³å‡è€—æ—¶: {avg_time:.2f}s")
        
        return tokenizer, avg_time
        
    except Exception as e:
        print(f"   âš ï¸ æ— æ³•åŠ è½½ base æ¨¡å‹: {e}")
        print(f"   æç¤º: å…ˆä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°")
        return None, None


def apply_torch_compile(model):
    """åº”ç”¨ Torch Compile ä¼˜åŒ–"""
    print("\nğŸš€ åº”ç”¨ Torch Compile ä¼˜åŒ–...")
    
    try:
        import torch
        if hasattr(torch, 'compile'):
            compiled_model = torch.compile(
                model,
                mode="reduce-overhead",
                fullgraph=False  # å…¼å®¹æ€§æ›´å¥½
            )
            print("   âœ… Torch Compile å·²å¯ç”¨")
            return compiled_model
        else:
            print("   âš ï¸ PyTorch ç‰ˆæœ¬ä¸æ”¯æŒ compile (éœ€è¦ 2.0+)")
            return model
    except Exception as e:
        print(f"   âš ï¸ Torch Compile å¤±è´¥: {e}")
        return model


def main():
    parser = argparse.ArgumentParser(description="FunASR å¿«é€Ÿä¼˜åŒ–æµ‹è¯•")
    parser.add_argument(
        "--encoder-path",
        default="/model/Step-Audio-Tokenizer",
        help="éŸ³é¢‘ç¼–ç å™¨è·¯å¾„"
    )
    parser.add_argument(
        "--test-audio",
        default="/app/examples/zero_shot_en_prompt.wav",
        help="æµ‹è¯•éŸ³é¢‘æ–‡ä»¶"
    )
    parser.add_argument(
        "--test-lightweight",
        action="store_true",
        help="æµ‹è¯•è½»é‡çº§æ¨¡å‹ï¼ˆparaformer-baseï¼‰"
    )
    parser.add_argument(
        "--benchmark-runs",
        type=int,
        default=3,
        help="åŸºå‡†æµ‹è¯•è¿è¡Œæ¬¡æ•°"
    )
    
    args = parser.parse_args()
    
    print("="*80)
    print("ğŸ¯ FunASR å¿«é€Ÿä¼˜åŒ–æµ‹è¯•")
    print("="*80)
    
    # 1. å¯ç”¨ TF32
    enable_tf32()
    
    # 2. åŠ è½½å½“å‰æ¨¡å‹å¹¶æµ‹è¯•
    print(f"\nğŸ“¦ åŠ è½½å½“å‰æ¨¡å‹...")
    tokenizer_original = StepAudioTokenizer(
        encoder_path=args.encoder_path,
        model_source=ModelSource.AUTO
    )
    
    baseline_time = benchmark_encoder(
        tokenizer_original,
        args.test_audio,
        args.benchmark_runs
    )
    
    print(f"\n{'='*80}")
    print(f"ğŸ“Š åŸºçº¿æ€§èƒ½ï¼ˆparaformer-large + TF32ï¼‰")
    print(f"{'='*80}")
    print(f"å¹³å‡è€—æ—¶: {baseline_time:.2f}s")
    
    # 3. æµ‹è¯•è½»é‡çº§æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
    if args.test_lightweight:
        tokenizer_light, light_time = test_lightweight_model(
            args.encoder_path,
            args.test_audio
        )
        
        if light_time is not None:
            speedup = baseline_time / light_time
            print(f"\n{'='*80}")
            print(f"ğŸ“Š è½»é‡æ¨¡å‹æ€§èƒ½ï¼ˆparaformer-base + TF32ï¼‰")
            print(f"{'='*80}")
            print(f"å¹³å‡è€—æ—¶: {light_time:.2f}s")
            print(f"æé€Ÿ: {speedup:.2f}x")
            
            if speedup > 1.5:
                print(f"\nâœ… å»ºè®®åˆ‡æ¢åˆ° paraformer-baseï¼")
                print(f"   é¢„æœŸæ€»æµç¨‹æé€Ÿ: {(baseline_time - light_time) / 24 * 100:.0f}%")
            else:
                print(f"\nâš ï¸ æé€Ÿä¸æ˜æ˜¾ï¼Œä¿æŒä½¿ç”¨ paraformer-large")
    
    # 4. æ€»ç»“å»ºè®®
    print(f"\n{'='*80}")
    print(f"ğŸ’¡ ä¼˜åŒ–å»ºè®®")
    print(f"{'='*80}")
    
    print("\nç«‹å³å¯åšï¼ˆä¿®æ”¹ tokenizer.pyï¼‰ï¼š")
    print("1. åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ  TF32 é…ç½®ï¼š")
    print("   ```python")
    print("   torch.backends.cuda.matmul.allow_tf32 = True")
    print("   torch.backends.cudnn.allow_tf32 = True")
    print("   ```")
    
    print("\n2. ä¼˜åŒ– ONNX Runtime é…ç½®ï¼ˆç¬¬ 62 è¡Œï¼‰ï¼š")
    print("   ```python")
    print("   session_option.intra_op_num_threads = 4  # ä» 1 æ”¹ä¸º 4")
    print("   session_option.inter_op_num_threads = 2")
    print("   session_option.execution_mode = onnxruntime.ExecutionMode.ORT_PARALLEL")
    print("   ```")
    
    if args.test_lightweight:
        print("\n3. åˆ‡æ¢åˆ°è½»é‡æ¨¡å‹ï¼ˆç¬¬ 22 è¡Œï¼‰ï¼š")
        print("   ```python")
        print('   funasr_model_id="damo/speech_paraformer-base_asr_nat-zh-cn-16k-common-vocab8404"')
        print("   ```")
    
    print("\né¢„æœŸæ€»ä½“æ•ˆæœï¼š")
    estimated_speedup = 1.2  # TF32 + ONNX ä¼˜åŒ–
    if args.test_lightweight and light_time is not None:
        estimated_speedup = baseline_time / light_time
    
    new_total_time = 24 - (baseline_time - baseline_time / estimated_speedup)
    improvement = (24 - new_total_time) / 24 * 100
    
    print(f"  - ç¼–ç æ—¶é—´: {baseline_time:.1f}s â†’ {baseline_time / estimated_speedup:.1f}s")
    print(f"  - æ€»æµç¨‹æ—¶é—´: 24s â†’ {new_total_time:.1f}s")
    print(f"  - æ€§èƒ½æå‡: {improvement:.0f}%")
    
    print(f"\nğŸ“š è¯¦ç»†ä¼˜åŒ–æŒ‡å—:")
    print(f"  - docs/funasr-optimization-guide.md")


if __name__ == "__main__":
    main()
